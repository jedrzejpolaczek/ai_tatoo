{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DYSKUSJA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTANIE: Czego potrzebujemy?\n",
    "ODPOWIEDZ: Uczenie nienadzorowanego!\n",
    "\n",
    "PYTANIE: Czemu?\n",
    "ODPOWIEDZ: ...\n",
    "\n",
    "PYTANIE: Czego użyjemy?\n",
    "ODPOWIEDZ: Używamy GAN.\n",
    "\n",
    "PYTANIE: Czemu GAN?\n",
    "ODPOWIEDZ: Ponieważ ogólna zasada jest taka, że próbujemy coś wygenerować i mamy detykowanego agenta, który mówi czy to się udało czy nie.\n",
    "Brzmi ok do uczenia generowania obrazów już istniejących.\n",
    "\n",
    "PYTANIE: W takim razie jak to działa że tworzy nowe obrazy?\n",
    "ODPOWIEDZ: Mamy w sumie dwa modele, jeden ma generować np. kwiatki, a drugi ma mówić czy to co dostaje do kwiatek. Jak model rozpoznający kwiatki powie \"to nie kwiatek\" to model generujący się updatuje, a jak model rozpoznający powie \"to totalnie jest kwiatek\" na to co dał model generujący, to model rozpoznający się updatuje i tak w kółko...\n",
    "Na sam koniec mamy model, który dostając obraz na wejście \"przewiduje\" co może być na wyjściu, czyli na nasze, będzie tworzył obraz jaki \"być powinien\" na bazie tego czego się nauczył.\n",
    "\n",
    "PYTANIE: A jakie sieci w tym GAN pan ma?\n",
    "ODPOWIEDZ: convolutional neural network (CNN)\n",
    "\n",
    "PYTANIE: Czemu?\n",
    "ODPOWIEDŹ: CNN mogą uczyć się na \"historii\" czyli trochę tak jakby to był film o kwiatkach i model będzie przewidywał następną klatkę tego filmu o kwiatkach. Żeby to zrobić musi coś wygenerować i nam zaproponować. To tutaj mamy część \"twórczą\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCHEMAT ROZWIĄZANIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narazie ta część bazuje/jest z książki \"Deep Learning Praca z językiem Python i biblioteką Kares\" autorstwa Francois Chollet:\n",
    "1. Sieć generatora \"generator\" mapuje wektor o kształcie \"latent_dim\" na obraz o kształcie (32, 32, 3).\n",
    "2. Sieć dyskryminatora \"discriminator\" mapuje obraz o kształcie (32, 32, 3) na binarną wartość określającą prawdopodobieństwo tego, że obraz jest prawdziwy.\n",
    "3. Sieć \"gan\" tworzy łańcuch  skłądajaćy się z generatora i dyskryminatora (gan(x)=discriminator(generator(x))). SIeć gan mapuje wektory niejawnej przestrzeni na oceny realizmu wystawiane przez dyskryminator.\n",
    "4. Trenujemy dyskryminator przy użyciu przykładów prawdziwych i wygenerowanych przez generator, oznaczonych etykietami, tak jakbyśmy trenowali zwykły model klasyfikacji obrazów.\n",
    "5. W celu wytrenowania generatora korzystamy z gradientów wag generatora w odniesieniu do straty modelu \"gan\". \n",
    "Inaczej (rozwijajac): W zwiazku z tym każdy krok trenowania ma modyfikować wagi generatora tak, aby zwiększyć prawdopodobieństwo zaklasyfikowania wygenerowanych obrazów jako prawdziwych. \n",
    "Inaczej (upraszczająć): Trenujemy generator tak by był w stanie oszukać dyskryminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT BIBLIOTEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARTOŚĆI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zmienne opisujące obrazy\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR\n",
    "\n",
    "Do stworzenia modelu generatora użyjemy konwoluncyjnej sieci neuronowej (ang. Convolutional Neural Network - CNN).\n",
    "\n",
    "Wybrałem ten rodzaj sieci ze względu na jej dużą efektywność rozwiązywaniu problemów widzenia maszynowego.\n",
    "Jej efektywność polega na tym, że w przeciwieństwie do klasycznych sieci gęstych (ang. Dense), które uczą się wzorców globalnych, sieci konwoluncyjne uczą się wzorców lokalnych (np. krawędzie, zaokrąglenia itd.). Kolejne warstwy rozpoznają coraz bardziej skomplikowane wzorce (np. uszy, nosy itp.).\n",
    "\n",
    "Skoro konwoluncyjna sieć neuronowa radzi sobie z rozpoznawaniem obrazów, może będzie dobrze działać przy ich generowaniu? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_40 (InputLayer)       [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 32768)             1081344   \n",
      "                                                                 \n",
      " leaky_re_lu_159 (LeakyReLU)  (None, 32768)            0         \n",
      "                                                                 \n",
      " reshape_30 (Reshape)        (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_157 (Conv2D)         (None, 32, 32, 256)       205056    \n",
      "                                                                 \n",
      " conv2d_158 (Conv2D)         (None, 32, 32, 256)       1048832   \n",
      "                                                                 \n",
      " leaky_re_lu_161 (LeakyReLU)  (None, 32, 32, 256)      0         \n",
      "                                                                 \n",
      " conv2d_159 (Conv2D)         (None, 32, 32, 256)       1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_162 (LeakyReLU)  (None, 32, 32, 256)      0         \n",
      "                                                                 \n",
      " conv2d_160 (Conv2D)         (None, 32, 32, 256)       1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_163 (LeakyReLU)  (None, 32, 32, 256)      0         \n",
      "                                                                 \n",
      " conv2d_161 (Conv2D)         (None, 32, 32, 3)         37635     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,650,179\n",
      "Trainable params: 5,650,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# INPUT LAYER: Warstwa wejściowa generatora\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# HIDDEN LAYERS: Pozostałe warstwy modelu generatora, uczące się wzorców obrazów (CNN)\n",
    "# CO ROBIMY? \n",
    "# BO: Zmieniamy obiekt wejściowy w 128 kanałową mapę cech 16x16\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: By gęstwa warstwa sieci neuronowej mogła przetworzyć obraz\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((32, 32, 32))(x)  # Tworzymy \n",
    "\n",
    "# CO ROBIMY?\n",
    "# BO: Standardowo trenujemy rozpoznawanie obrazów\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: Conv2D to 2 wymiarowa sieć konwoluncyjna, generalnie dająca dobre wyniki w rozpoznawaniu obrazów\n",
    "# CZEMU TAKIE WARTOŚĆI?\n",
    "# BO: \n",
    "# 256 kanałów, byśmy mogli wyłapać jak najwięcej cech.\n",
    "# 5 oznaczająca 5x5 mapa cech, ponieważ przy mniejszej moglibyśmy nie wyłapać bardziej ogólnych cech na których nam zależy\n",
    "# padding ustawiony na 'same' oznacza tyle, że włączamy padding. Padding to dopisywanie 0 z prawej/lewej lub dołu/góry do wektora.\n",
    "# \"same\" results in padding with zeros evenly to the left/right or up/down of the input. \n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "X = layers.LeakyReLU()(x)\n",
    "\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: Chcemy zwiększyć odrobinę obraz by lepiej wyłapać jego cechy.\n",
    "# When padding=\"same\" and strides=1, the output has the same size as the input.\n",
    "x = layers.Conv2D(256, 4, strides=1, padding='same')(x)  # Zwiększenie rozmiaru do 32x32. Jeśli padding jest włączony (ustawiony na 'same') i strides jest ustawione na 1 to obraz wyjściowy jest takiego samego rozmiaru co wejsciowy..\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# OUTPUT LAYER: Warstwa wyjściwowa generatora, która daje nam obraz. \n",
    "# Tworzy instancję generatora, która mapuje obiekt wejściowy o kształcie (latent_dim,) na obraz o kształcie (32, 32, 3)\n",
    "# CZEMU TANH?\n",
    "# BO: tanh to \"Hyperbolic tangent activation function\". DUNNO ;_______; \n",
    "# JAK TO DZIAŁA?\n",
    "# Sieć konwoluncyjna stworzy obraz wejściowy na kanałach o wartości \"channels\" z użyciem filtra o wielkości 7. Dla obrazów 32 na 32 taki filtr nie spowoduje utraty pikseli.\n",
    "# Dla pewności dodajemy padding.\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "\n",
    "# DEKLARACJA MODELU GENERATORA\n",
    "# JAK TO GENERUJE OBRAZ?\n",
    "# BO: ...\n",
    "generator = keras.models.Model(generator_input, x)  # Generuje jednokanałową mapę cech o rozmiarze 32x32 (rozmiar ten jesy taki sam jak rozmiar obraz,ow wchodzących w skład zbioru CIFAR10)\n",
    "generator.summary()\n",
    "\n",
    "# OPTYMALIZACJA MODELU GENERATORA\n",
    "# nie ma takiej potrzeby, ponieważ model generatora jest zawarty w modelu dyskryminatora\n",
    "# wystarczy że ustawimy optymalizatora dla modelu dyskryminatora.\n",
    "# KOMPILACJA MODELU GENERATORA\n",
    "# j/w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyskryminator\n",
    "\n",
    "Uzasadnienie podobne jak do generatora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_41 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_162 (Conv2D)         (None, 30, 30, 128)       3584      \n",
      "                                                                 \n",
      " leaky_re_lu_164 (LeakyReLU)  (None, 30, 30, 128)      0         \n",
      "                                                                 \n",
      " conv2d_163 (Conv2D)         (None, 14, 14, 128)       262272    \n",
      "                                                                 \n",
      " leaky_re_lu_165 (LeakyReLU)  (None, 14, 14, 128)      0         \n",
      "                                                                 \n",
      " conv2d_164 (Conv2D)         (None, 6, 6, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_166 (LeakyReLU)  (None, 6, 6, 128)        0         \n",
      "                                                                 \n",
      " conv2d_165 (Conv2D)         (None, 2, 2, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_167 (LeakyReLU)  (None, 2, 2, 128)        0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# INPUT LAYER\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "# HIDDEN LAYERS\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)  # Powiększamy obraz by nie stracić szczegółów przy ocenie prawidziwości obrazka\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)  # j/w\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)  # j/w\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x)  # Ważna warstwa bo GAN łątwo wpada w optimum lokalne przy doborze wag\n",
    "\n",
    "# OUTPUT LAYER\n",
    "x = layers.Dense(1, activation='sigmoid')(x)  # klasyczna warstwa klasyfikacji binarnej\n",
    "\n",
    "# DEKLARACJA MODELU DYSKRYMINATORA\n",
    "# Tworzenie instancji modelu dyskryminatora zamieniajacego obiekt wejściowy mający kształt (32, 32, 3) na wynik klasyfikacji binarnej określającej prawdziwość obrazu\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedrz\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# OPTYMALIZACJA MODELU DYSKRYMINATORA\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(\n",
    "    lr=0.0008,\n",
    "    clipvalue=1.0,  # Optymalizator korzytsa z mechanizmu ucinania wartości gradientu\n",
    "    decay=1e-8  # W celu uzyskania stabilnego przebiegu procesu trenowania korzystamy z parametru rozkłądu współczynnika uczenia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMPILACJA MODELU DYSKRYMINATORA\n",
    "discriminator.compile(\n",
    "    optimizer=discriminator_optimizer,\n",
    "    loss='binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIEĆ Z PRZECIWNIKIEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False  # Umożliwiamy trenowanie wag dyskryminatora (tylko w modelu gan)\n",
    "\n",
    "# INPUT LAYER\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# HIDDEN LAYERS\n",
    "pass\n",
    "\n",
    "# OUTPUT LAYER\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "\n",
    "# DEKLARACJA MODELU GAN\n",
    "gan = keras.models.Model(gan_input, gan_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTYMALIZACJA MODELU GAN\n",
    "gan_optimizer = keras.optimizers.RMSprop(\n",
    "    lr=0.0004,\n",
    "    clipvalue=1.0,\n",
    "    decay=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMPILACJA MODELU GAN\n",
    "gan.compile(\n",
    "    optimizer=gan_optimizer,\n",
    "    loss='binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRENOWANIE SIECI GAN\n",
    "1. Wybieramy losowe punkty z niejawnej przestrzeni (losowy szum)\n",
    "2. Użyj generatora w celu wygenerowania obrazów zawierających losowy szum\n",
    "3. Połącz wygenerowane obrazy z prawdziwymi\n",
    "4. Wytrenuj dyskryminator przy użyciu wylosowanych obrazów z etykietami określajacymi prawdziwość obrazów\n",
    "5. Wybierz kolejne losowe punkty z niejawnej przestrzeni\n",
    "6. Trenuj model gan w tym celu, aby wszystkie obrazy były uznawane przez dyskryminator za prawdziwe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "\n",
    "(x_train, y_train), (_, _) = tensorflow.keras.datasets.cifar10.load_data()  # Ładowanie zbioru CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[y_train.flatten() == 6]  # Wybór obrazów żab (klasa numer 6)\n",
    "\n",
    "x_train = x_train.reshape(\n",
    "    (x_train.shape[0],) +\n",
    "    (height, width, channels)\n",
    ").astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = \"gan_images\"\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJądro Kernel uległo awarii podczas wykonywania kodu w bieżącej komórce lub w poprzedniej komórce. Przejrzyj kod w komórkach, aby zidentyfikować możliwą przyczynę awarii. Kliknij <a href='https://aka.ms/vscodeJupyterKernelCrash'>tutaj</a>, aby uzyskać więcej informacji. W celu uzyskania dalszych szczegółów, wyświetl <a href='command:jupyter.viewOutput'>log</a> Jupyter."
     ]
    }
   ],
   "source": [
    "for step in range(iterations):\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))  # Próbkowanie losowych punktów z niejawnej przestrzeni\n",
    "\n",
    "    generated_images = generator.predict(random_latent_vectors)  # Dekodowanie punktów w celu wygenerowania sztucznych obrazów\n",
    "\n",
    "    # Łaczenie obrazów sztucznych z prawdziwymi\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "\n",
    "    # Tworzenie etykiet umożliwiających odróżnienie obrazów prawdziwych od sztucznych\n",
    "    labels = np.concatenate(\n",
    "        [np.ones((batch_size, 1)),\n",
    "        np.zeros((batch_size, 1))])\n",
    "    \n",
    "    labels += 0.55 * np.random.random(labels.shape)  # WAŻNE: wprowadzanie losowego szumu etykiet\n",
    "\n",
    "    # Trenowanie dyskryminatora\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "    # Losowe próbkowanie punktów stwierdzających oryginalność przestrzeni\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) \n",
    "\n",
    "    # Tworzenie fałszywych etykiet stwierdzających oryginalność obrazów\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "\n",
    "    # Trenowanie generatora przy użyciu modelu gan i zamrożeniu dyskryminatora\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "\n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    \n",
    "    # Okazjonalny zapis danych i generowanie wykresów (co 100 kroków algorytmu)\n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('gan.h5')  # Zapis wag modelu\n",
    "\n",
    "        print('Strata dysryminatora w kroku %s: %s' % (step, d_loss))\n",
    "        print('strata przeciwna: %s: %s' % (step, a_loss))\n",
    "        \n",
    "        # Zapis jednego wygenerowanego obrazu\n",
    "        img = tensorflow.keras.utils.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated_frog_' + str(step) + '.png'))\n",
    "\n",
    "        # Zapis jednego prawdziwego obrazu w celach porównawczych\n",
    "        img = tensorflow.keras.utils.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_frog_' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTATKI\n",
    "## Rule of thumbs\n",
    "1. Ostatnia warstwa to *tanh*.\n",
    "2. Próbkowanie punktów za pomocą rozkłądu Gaussa.\n",
    "3. Dodajemy dużo losowości do procesu losowania (bo GAN ma tendencję do wpadania w optimum lokalne, BARDZO), np. drop wag i szum etykiet.\n",
    "4. Rzadkie gradienty są fe. Co robi rzadki gradient? Np. Maxpooling i ReLU. Wiec zamiast maxpooling dajemy krokową konwolucję. Zamiast ReLU dajemy LeakyReLU.\n",
    "5. Zawsze gdy używamy Conv2DTranpose lub Conv2D zastosujemy rozmiar jądra podzielony przez rozmiar kroku. POmoże nam to uniknąć artefaktów takich jak \"szachownica\" na generowanym obrazie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47ad5f08fe757f55c3f60a32a8074f9dbcb5289c034c2b1e952976897fc4f86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
