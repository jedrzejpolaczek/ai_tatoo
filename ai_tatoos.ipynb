{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DYSKUSJA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTANIE: Czego potrzebujemy?\n",
    "ODPOWIEDZ: Uczenie nienadzorowanego!\n",
    "\n",
    "PYTANIE: Czemu?\n",
    "ODPOWIEDZ: ...\n",
    "\n",
    "PYTANIE: Czego użyjemy?\n",
    "ODPOWIEDZ: Używamy GAN.\n",
    "\n",
    "PYTANIE: Czemu GAN?\n",
    "ODPOWIEDZ: Ponieważ ogólna zasada jest taka, że próbujemy coś wygenerować i mamy detykowanego agenta, który mówi czy to się udało czy nie.\n",
    "Brzmi ok do uczenia generowania obrazów już istniejących.\n",
    "\n",
    "PYTANIE: W takim razie jak to działa że tworzy nowe obrazy?\n",
    "ODPOWIEDZ: Mamy w sumie dwa modele, jeden ma generować np. kwiatki, a drugi ma mówić czy to co dostaje do kwiatek. Jak model rozpoznający kwiatki powie \"to nie kwiatek\" to model generujący się updatuje, a jak model rozpoznający powie \"to totalnie jest kwiatek\" na to co dał model generujący, to model rozpoznający się updatuje i tak w kółko...\n",
    "Na sam koniec mamy model, który dostając obraz na wejście \"przewiduje\" co może być na wyjściu, czyli na nasze, będzie tworzył obraz jaki \"być powinien\" na bazie tego czego się nauczył.\n",
    "\n",
    "PYTANIE: A jakie sieci w tym GAN pan ma?\n",
    "ODPOWIEDZ: convolutional neural network (CNN)\n",
    "\n",
    "PYTANIE: Czemu?\n",
    "ODPOWIEDŹ: CNN mogą uczyć się na \"historii\" czyli trochę tak jakby to był film o kwiatkach i model będzie przewidywał następną klatkę tego filmu o kwiatkach. Żeby to zrobić musi coś wygenerować i nam zaproponować. To tutaj mamy część \"twórczą\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCHEMAT ROZWIĄZANIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narazie ta część bazuje/jest z książki \"Deep Learning Praca z językiem Python i biblioteką Kares\" autorstwa Francois Chollet:\n",
    "1. Sieć generatora \"generator\" mapuje wektor o kształcie \"latent_dim\" na obraz o kształcie (32, 32, 3).\n",
    "2. Sieć dyskryminatora \"discriminator\" mapuje obraz o kształcie (32, 32, 3) na binarną wartość określającą prawdopodobieństwo tego, że obraz jest prawdziwy.\n",
    "3. Sieć \"gan\" tworzy łańcuch  skłądajaćy się z generatora i dyskryminatora (gan(x)=discriminator(generator(x))). SIeć gan mapuje wektory niejawnej przestrzeni na oceny realizmu wystawiane przez dyskryminator.\n",
    "4. Trenujemy dyskryminator przy użyciu przykładów prawdziwych i wygenerowanych przez generator, oznaczonych etykietami, tak jakbyśmy trenowali zwykły model klasyfikacji obrazów.\n",
    "5. W celu wytrenowania generatora korzystamy z gradientów wag generatora w odniesieniu do straty modelu \"gan\". \n",
    "Inaczej (rozwijajac): W zwiazku z tym każdy krok trenowania ma modyfikować wagi generatora tak, aby zwiększyć prawdopodobieństwo zaklasyfikowania wygenerowanych obrazów jako prawdziwych. \n",
    "Inaczej (upraszczająć): Trenujemy generator tak by był w stanie oszukać dyskryminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32768)             1081344   \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 32768)             0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 256)       819456    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 256)         1048832   \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 256)         1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 256)         1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 3)           37635     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Zmienne opisujące obrazy\n",
    "latent_dim = 32\n",
    "heigh = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "# INPUT LAYER: Warstwa wejściowa generatora\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# HIDDEN LAYER: Pozostałe warstwy modelu generatora\n",
    "# CO ROBIMY? \n",
    "# BO: Zmieniamy obiekt wejściowy w 128 kanałową mapę cech 16x16\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: \n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "# CO ROBIMY?\n",
    "# BO: Standardowo trenujemy \"przewidywanie\" jakie powinno być \"wyjście\" (jak w klatkach filmowych)\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: \n",
    "# CZEMU CONV2D?\n",
    "# CZMEU PADIDNG? CZEMU PADDING \"SAME\"?\n",
    "# CZEMU TAKIE WARTOŚĆI?\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "X = layers.LeakyReLU()(x)\n",
    "\n",
    "# PO CO TO?\n",
    "x = layers.Conv2D(256, 4, strides=2, padding='same')(x)  # Zwiększenie rozmiaru do 32x32\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# OUTPUT LAYER: Warstwa wyjściwowa generatora, która daje nam obraz. \n",
    "# Tworzy instancję generatora, która mapuje obiekt wejściowy o kształcie (latent_dim,) na obraz o kształcie (32, 32, 3)\n",
    "# CZEMU TANH? JAK TO DZIAŁA?\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)  # Generuje jednokanałową mapę cech o rozmiarze 32x32 (rozmiar ten jesy taki sam jak rozmiar obraz,ow wchodzących w skład zbioru CIFAR10)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTATKI\n",
    "## Rule of thumbs\n",
    "1. Ostatnia warstwa to *tanh*.\n",
    "2. Próbkowanie punktów za pomocą rozkłądu Gaussa.\n",
    "3. Dodajemy dużo losowości do procesu losowania (bo GAN ma tendencję do wpadania w optimum lokalne, BARDZO), np. drop wag i szum etykiet.\n",
    "4. Rzadkie gradienty są fe. Co robi rzadki gradient? Np. Maxpooling i ReLU. Wiec zamiast maxpooling dajemy krokową konwolucję. Zamiast ReLU dajemy LeakyReLU.\n",
    "5. Zawsze gdy używamy Conv2DTranpose lub Conv2D zastosujemy rozmiar jądra podzielony przez rozmiar kroku. POmoże nam to uniknąć artefaktów takich jak \"szachownica\" na generowanym obrazie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fca86f6b1e83c790fbbe2f37ee7991c335ca66a3f5db41e91d95f1f3ff0270ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
