{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DYSKUSJA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PYTANIE: Czego potrzebujemy?\n",
    "ODPOWIEDZ: Uczenie nienadzorowanego!\n",
    "\n",
    "PYTANIE: Czemu?\n",
    "ODPOWIEDZ: ...\n",
    "\n",
    "PYTANIE: Czego użyjemy?\n",
    "ODPOWIEDZ: Używamy GAN.\n",
    "\n",
    "PYTANIE: Czemu GAN?\n",
    "ODPOWIEDZ: Ponieważ ogólna zasada jest taka, że próbujemy coś wygenerować i mamy detykowanego agenta, który mówi czy to się udało czy nie.\n",
    "Brzmi ok do uczenia generowania obrazów już istniejących.\n",
    "\n",
    "PYTANIE: W takim razie jak to działa że tworzy nowe obrazy?\n",
    "ODPOWIEDZ: Mamy w sumie dwa modele, jeden ma generować np. kwiatki, a drugi ma mówić czy to co dostaje do kwiatek. Jak model rozpoznający kwiatki powie \"to nie kwiatek\" to model generujący się updatuje, a jak model rozpoznający powie \"to totalnie jest kwiatek\" na to co dał model generujący, to model rozpoznający się updatuje i tak w kółko...\n",
    "Na sam koniec mamy model, który dostając obraz na wejście \"przewiduje\" co może być na wyjściu, czyli na nasze, będzie tworzył obraz jaki \"być powinien\" na bazie tego czego się nauczył.\n",
    "\n",
    "PYTANIE: A jakie sieci w tym GAN pan ma?\n",
    "ODPOWIEDZ: convolutional neural network (CNN)\n",
    "\n",
    "PYTANIE: Czemu?\n",
    "ODPOWIEDŹ: CNN mogą uczyć się na \"historii\" czyli trochę tak jakby to był film o kwiatkach i model będzie przewidywał następną klatkę tego filmu o kwiatkach. Żeby to zrobić musi coś wygenerować i nam zaproponować. To tutaj mamy część \"twórczą\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCHEMAT ROZWIĄZANIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narazie ta część bazuje/jest z książki \"Deep Learning Praca z językiem Python i biblioteką Kares\" autorstwa Francois Chollet:\n",
    "1. Sieć generatora \"generator\" mapuje wektor o kształcie \"latent_dim\" na obraz o kształcie (32, 32, 3).\n",
    "2. Sieć dyskryminatora \"discriminator\" mapuje obraz o kształcie (32, 32, 3) na binarną wartość określającą prawdopodobieństwo tego, że obraz jest prawdziwy.\n",
    "3. Sieć \"gan\" tworzy łańcuch  skłądajaćy się z generatora i dyskryminatora (gan(x)=discriminator(generator(x))). SIeć gan mapuje wektory niejawnej przestrzeni na oceny realizmu wystawiane przez dyskryminator.\n",
    "4. Trenujemy dyskryminator przy użyciu przykładów prawdziwych i wygenerowanych przez generator, oznaczonych etykietami, tak jakbyśmy trenowali zwykły model klasyfikacji obrazów.\n",
    "5. W celu wytrenowania generatora korzystamy z gradientów wag generatora w odniesieniu do straty modelu \"gan\". \n",
    "Inaczej (rozwijajac): W zwiazku z tym każdy krok trenowania ma modyfikować wagi generatora tak, aby zwiększyć prawdopodobieństwo zaklasyfikowania wygenerowanych obrazów jako prawdziwych. \n",
    "Inaczej (upraszczająć): Trenujemy generator tak by był w stanie oszukać dyskryminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT BIBLIOTEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARTOŚĆI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zmienne opisujące obrazy\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR\n",
    "\n",
    "Do stworzenia modelu generatora użyjemy konwoluncyjnej sieci neuronowej (ang. Convolutional Neural Network - CNN).\n",
    "\n",
    "Wybrałem ten rodzaj sieci ze względu na jej dużą efektywność rozwiązywaniu problemów widzenia maszynowego.\n",
    "Jej efektywność polega na tym, że w przeciwieństwie do klasycznych sieci gęstych (ang. Dense), które uczą się wzorców globalnych, sieci konwoluncyjne uczą się wzorców lokalnych (np. krawędzie, zaokrąglenia itd.). Kolejne warstwy rozpoznają coraz bardziej skomplikowane wzorce (np. uszy, nosy itp.).\n",
    "\n",
    "Skoro konwoluncyjna sieć neuronowa radzi sobie z rozpoznawaniem obrazów, może będzie dobrze działać przy ich generowaniu? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 32)]              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32768)             1081344   \n",
      "                                                                 \n",
      " leaky_re_lu_33 (LeakyReLU)  (None, 32768)             0         \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 16, 16, 256)       819456    \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 8, 8, 256)         1048832   \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 8, 8, 256)         1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_36 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 8, 8, 256)         1638656   \n",
      "                                                                 \n",
      " leaky_re_lu_37 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 8, 8, 3)           37635     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# INPUT LAYER: Warstwa wejściowa generatora\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# HIDDEN LAYERS: Pozostałe warstwy modelu generatora, uczące się wzorców obrazów (CNN)\n",
    "# CO ROBIMY? \n",
    "# BO: Zmieniamy obiekt wejściowy w 128 kanałową mapę cech 16x16\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: By gęstwa warstwa sieci neuronowej mogła przetworzyć obraz\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)  # Tworzymy \n",
    "\n",
    "# CO ROBIMY?\n",
    "# BO: Standardowo trenujemy rozpoznawanie obrazów\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: Conv2D to 2 wymiarowa sieć konwoluncyjna, generalnie dająca dobre wyniki w rozpoznawaniu obrazów\n",
    "# CZEMU TAKIE WARTOŚĆI?\n",
    "# BO: \n",
    "# 256 kanałów, byśmy mogli wyłapać jak najwięcej cech.\n",
    "# 5 oznaczająca 5x5 mapa cech, ponieważ przy mniejszej moglibyśmy nie wyłapać bardziej ogólnych cech na których nam zależy\n",
    "# padding ustawiony na 'same' oznacza tyle, że włączamy padding. Padding to dopisywanie 0 z prawej/lewej lub dołu/góry do wektora.\n",
    "# \"same\" results in padding with zeros evenly to the left/right or up/down of the input. \n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "X = layers.LeakyReLU()(x)\n",
    "\n",
    "# CZEMU TO TU JEST?\n",
    "# BO: Chcemy zwiększyć odrobinę obraz by lepiej wyłapać jego cechy.\n",
    "# When padding=\"same\" and strides=1, the output has the same size as the input.\n",
    "x = layers.Conv2D(256, 4, strides=2, padding='same')(x)  # Zwiększenie rozmiaru do 32x32. Jeśli padding jest włączony (ustawiony na 'same') i strides jest ustawione na 1 to obraz wyjściowy jest takiego samego rozmiaru co wejsciowy. Przy sutawieniu na 2 obraz wyjściowy jest dwukrotnie większy jak wejściowy.\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# OUTPUT LAYER: Warstwa wyjściwowa generatora, która daje nam obraz. \n",
    "# Tworzy instancję generatora, która mapuje obiekt wejściowy o kształcie (latent_dim,) na obraz o kształcie (32, 32, 3)\n",
    "# CZEMU TANH?\n",
    "# BO: tanh to \"Hyperbolic tangent activation function\". DUNNO ;_______; \n",
    "# JAK TO DZIAŁA?\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "\n",
    "# DEKLARACJA MODELU GENERATORA\n",
    "# JAK TO GENERUJE OBRAZ?\n",
    "# BO: ...\n",
    "generator = keras.models.Model(generator_input, x)  # Generuje jednokanałową mapę cech o rozmiarze 32x32 (rozmiar ten jesy taki sam jak rozmiar obraz,ow wchodzących w skład zbioru CIFAR10)\n",
    "generator.summary()\n",
    "\n",
    "# OPTYMALIZACJA MODELU GENERATORA\n",
    "# nie ma takiej potrzeby, ponieważ model generatora jest zawarty w modelu dyskryminatora\n",
    "# wystarczy że ustawimy optymalizatora dla modelu dyskryminatora.\n",
    "# KOMPILACJA MODELU GENERATORA\n",
    "# j/w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyskryminator\n",
    "\n",
    "Uzasadnienie podobne jak do generatora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 30, 30, 128)       3584      \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 30, 30, 128)       0         \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 14, 14, 128)       262272    \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 6, 6, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_40 (LeakyReLU)  (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 2, 2, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_41 (LeakyReLU)  (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# INPUT LAYER\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "# HIDDEN LAYERS\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)  # Powiększamy obraz by nie stracić szczegółów przy ocenie prawidziwości obrazka\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)  # j/w\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)  # j/w\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x)  # Ważna warstwa bo GAN łątwo wpada w optimum lokalne przy doborze wag\n",
    "\n",
    "# OUTPUT LAYER\n",
    "x = layers.Dense(1, activation='sigmoid')(x)  # klasyczna warstwa klasyfikacji binarnej\n",
    "\n",
    "# DEKLARACJA MODELU DYSKRYMINATORA\n",
    "# Tworzenie instancji modelu dyskryminatora zamieniajacego obiekt wejściowy mający kształt (32, 32, 3) na wynik klasyfikacji binarnej określającej prawdziwość obrazu\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedrz\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# OPTYMALIZACJA MODELU DYSKRYMINATORA\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(\n",
    "    lr=0.0008,\n",
    "    clipvalue=1.0,  # Optymalizator korzytsa z mechanizmu ucinania wartości gradientu\n",
    "    decay=1e-8  # W celu uzyskania stabilnego przebiegu procesu trenowania korzystamy z parametru rozkłądu współczynnika uczenia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMPILACJA MODELU DYSKRYMINATORA\n",
    "discriminator.compile(\n",
    "    optimizer=discriminator_optimizer,\n",
    "    loss='binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIEĆ Z PRZECIWNIKIEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 32, 32, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='input_9'), name='input_9', description=\"created by layer 'input_9'\"), but it was called on an input with incompatible shape (None, 8, 8, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv2d_39\" (type Conv2D).\n\nNegative dimension size caused by subtracting 4 from 2 for '{{node model_7/conv2d_39/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](model_7/leaky_re_lu_39/LeakyRelu, model_7/conv2d_39/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,128], [4,4,128,128].\n\nCall arguments received by layer \"conv2d_39\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 128), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Workspace\\PROGRAMOWANIE\\ai_tatoo\\ai_tatoos.ipynb Komórka 16\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/PROGRAMOWANIE/ai_tatoo/ai_tatoos.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# OUTPUT LAYER\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/PROGRAMOWANIE/ai_tatoo/ai_tatoos.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m generator_layer \u001b[39m=\u001b[39m generator(gan_input)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/PROGRAMOWANIE/ai_tatoo/ai_tatoos.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m gan_output \u001b[39m=\u001b[39m discriminator(generator_layer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/PROGRAMOWANIE/ai_tatoo/ai_tatoos.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# DEKLARACJA MODELU GAN\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/PROGRAMOWANIE/ai_tatoo/ai_tatoos.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m gan \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mModel(gan_input, gan_output)\n",
      "File \u001b[1;32mc:\\Users\\jedrz\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jedrz\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1963\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1960\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1961\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m   1965\u001b[0m \u001b[39mreturn\u001b[39;00m c_op\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_39\" (type Conv2D).\n\nNegative dimension size caused by subtracting 4 from 2 for '{{node model_7/conv2d_39/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](model_7/leaky_re_lu_39/LeakyRelu, model_7/conv2d_39/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,128], [4,4,128,128].\n\nCall arguments received by layer \"conv2d_39\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 128), dtype=float32)"
     ]
    }
   ],
   "source": [
    "discriminator.trainable = False  # Umożliwiamy trenowanie wag dyskryminatora (tylko w modelu gan)\n",
    "\n",
    "# INPUT LAYER\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# HIDDEN LAYERS\n",
    "pass\n",
    "\n",
    "# OUTPUT LAYER\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "\n",
    "# DEKLARACJA MODELU GAN\n",
    "gan = keras.models.Model(gan_input, gan_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTYMALIZACJA MODELU GAN\n",
    "gan_optimizer = keras.optimizers.RMSprop(\n",
    "    lr=0.0004,\n",
    "    clipvalue=1.0,\n",
    "    decay=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMPILACJA MODELU GAN\n",
    "gan.compile(\n",
    "    optimizer=gan_optimizer,\n",
    "    loss='binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTATKI\n",
    "## Rule of thumbs\n",
    "1. Ostatnia warstwa to *tanh*.\n",
    "2. Próbkowanie punktów za pomocą rozkłądu Gaussa.\n",
    "3. Dodajemy dużo losowości do procesu losowania (bo GAN ma tendencję do wpadania w optimum lokalne, BARDZO), np. drop wag i szum etykiet.\n",
    "4. Rzadkie gradienty są fe. Co robi rzadki gradient? Np. Maxpooling i ReLU. Wiec zamiast maxpooling dajemy krokową konwolucję. Zamiast ReLU dajemy LeakyReLU.\n",
    "5. Zawsze gdy używamy Conv2DTranpose lub Conv2D zastosujemy rozmiar jądra podzielony przez rozmiar kroku. POmoże nam to uniknąć artefaktów takich jak \"szachownica\" na generowanym obrazie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fca86f6b1e83c790fbbe2f37ee7991c335ca66a3f5db41e91d95f1f3ff0270ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
